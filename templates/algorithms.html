{% extends "layout.html" %}
{% set active_page = "algopage" %}
{% block content %}
<!-- CODE -->
<div class="title">
    Prediction Classifiers
</div>

<div class="title2">
    Random Forest (RF)
</div>
<div style="background-color: gray; padding: 10px 2%;">
    <div class="textbox1">
        Random Forest splits the data into growing trees. Then using minute differences to learn which features are relevant, it's able to classify. <br> <br>
        The graph below (4.1) shows the <span style="color: darkgreen;">number of trees used in the forest (n
        estimators)</span>. We found that if n=8  our model predicts at the <span style="color: darkgreen;">highest accuracy</span>. <br> <br>
        <img src="{{ url_for('static', filename='images/rf_error.png') }}" alt="rf_error">
        <p>Figure 4.1: Graph showing the error value based on n estimators.</p>
    </div>

    <div class="textbox1">
        Random Forest got a <span style="color: orangered;">85.10%</span> prediction accuracy.
    </div>
</div>

<div class="title2" style="background-color: whitesmoke; color: black;">
    Decision Tree (DT)
</div>
<div style="background-color: whitesmoke; padding: 10px 2%;">
    <div class="textbox1">
        Decision trees use various algorithms which splits a node into two or more predecessor nodes.
        For every split, the algorithm calculates the information gain and entropy of every unused
        attribute. It will then select the highest attribute. Then, it splits again and repeats the process with the
        unused features. This algorithm continues until all attributes are utilized. <br> <br>
        The graph below (4.2) shows the <span style="color: darkgreen;">tree depth</span>
        that predicts at the <span style="color: darkgreen;">highest accuracy</span>. <br><br>
        <img src="{{ url_for('static', filename='images/dt_error.png') }}" alt="dt_error">
        <p>Figure 4.2: Shows the error values based off the tree depth when utilizing DT.</p>
    </div>
    
    <div class="textbox1">
        Decision Tree got a <span style="color: orangered;">82.211%</span> prediction accuracy.
    </div>
</div>

<div class="title2">
    K-Nearest Neighbor (KNN)
</div>
<div style="background-color: gray; padding: 10px 2%;">
    <div class="textbox1">
        KNN assumes that proximity is equivalent to similarity. The algorithm plots each sample and finds the K number of nearest samples. The majority classification of those samples is assigned to the new sample. <br> <br>
        The graph below (4.3) shows the <span style="color: darkgreen;">amount of nearest neighbors (k value)</span>
        that predicts at the <span style="color: darkgreen;">highest accuracy</span>. <br> <br>
        <img src="{{ url_for('static', filename='images/kn_error.png') }}" alt="kn_error">
        <p>Figure 4.3: Graph showing the highest accuracy based on K value.</p>
    </div>

    <div class="textbox1">
        K-Nearest Neighbor got a <span style="color: orangered;">70.192%</span> prediction accuracy.
    </div>
</div>

<div class="title2" style="background-color: whitesmoke; color: black;">
    Three more classifiers
</div>
<div class="gridcontainer" style="background-color: whitesmoke;">
    <div class="textbox1" style="grid-row: 1; grid-column: 1/2; font-size: large;">
        Ada Boost got a <span style="color: orangered;">75%</span> prediction accuracy.
    </div>
    <div class="textbox1" style="grid-row: 2; grid-column: 1/2; font-size: large;">
        Gaussian got a <span style="color: orangered;">69.712%</span> prediction accuracy.
    </div>

    <div class="textbox1" style="grid-row: 3; grid-column: 1/2; font-size: large;">
        XG Boost got a <span style="color: orangered;">81.731%</span> prediction accuracy.
    </div>
</div>
<!-- END CODE -->
{% endblock %}